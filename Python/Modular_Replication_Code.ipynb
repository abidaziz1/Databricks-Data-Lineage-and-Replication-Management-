{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664e17ae-24e1-453e-a2e0-d48a2cd2fdb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "#%run /Shared/MetaDatarepliaction_Backend_Code/Post_Replication_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be157588-5063-40a6-ae10-3b88e62d10ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text('user_input', \"[{'source_type':'dbfs_delta_Table','table_name':'Customer'},{'source_type':'sql_server','table_name':'dbo.employee'}]\")\n",
    "\n",
    "input = dbutils.widgets.get(\"user_input\")\n",
    "# dbutils.widgets.removeAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a80f81a-9c7d-46bf-8184-b905725decca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: \"[{'source_type':'dbfs_delta_Table','table_name':'Customer'},{'source_type':'sql_server','table_name':'dbo.Employee'}]\""
     ]
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "058c9ea4-a398-4e16-beea-bc383d3ade8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_files_list_dict={}\n",
    "for i in dbutils.fs.ls('dbfs:/databricks-datasets/tpch/delta-001/'):\n",
    "    if i.size ==0:\n",
    "        delta_files_list_dict[i.name[:-1].capitalize()]=i.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac14bed-7e70-489c-a8bc-a612035c94c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: {'Customer': 'dbfs:/databricks-datasets/tpch/delta-001/customer/',\n",
      " 'Lineitem': 'dbfs:/databricks-datasets/tpch/delta-001/lineitem/',\n",
      " 'Nation': 'dbfs:/databricks-datasets/tpch/delta-001/nation/',\n",
      " 'Orders': 'dbfs:/databricks-datasets/tpch/delta-001/orders/',\n",
      " 'Part': 'dbfs:/databricks-datasets/tpch/delta-001/part/',\n",
      " 'Partsupp': 'dbfs:/databricks-datasets/tpch/delta-001/partsupp/',\n",
      " 'Region': 'dbfs:/databricks-datasets/tpch/delta-001/region/',\n",
      " 'Supplier': 'dbfs:/databricks-datasets/tpch/delta-001/supplier/'}"
     ]
    }
   ],
   "source": [
    "delta_files_list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb427052-2b42-4c6c-9ada-33bf31488f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Blob Storage details\n",
    "storage_account_name = \"<storage-account-name>\"\n",
    "storage_account_key = \"<storage-account-access-key>\"  \n",
    "container_name = \"input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1326667e-6a5f-4e99-ba32-769699466b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting up the Spark configuration for Azure Blob Storage authentication\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63630819-c854-4bd0-aa81-1270727611ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_tables_list_dict = {}\n",
    "for i in dbutils.fs.ls(f\"wasbs://input@{storage_account_name}.blob.core.windows.net/Input_DB\"):\n",
    "    if i.size != 0:\n",
    "        csv_tables_list_dict[i.name[:-4].capitalize()] = i.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163b2a24-8f1d-4977-89ce-3bcf945f2659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[53]: {'Employees': 'wasbs://input@metadatamanagement02.blob.core.windows.net/Input_DB/employees.csv'}"
     ]
    }
   ],
   "source": [
    "csv_tables_list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4be3b0f-a11e-44be-b658-150b90c755e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_container_name = \"output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42251c69-5bbd-443c-92a3-bd15f9c558f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delta_file_replication(tab_name, delta_files_list_dict):\n",
    "    delta_tables_list = {}\n",
    "    # Direct path to your replication folder on Azure Blob Storage (without mounting)\n",
    "    replication_path = \"wasbs://output@metadatamanagement02.blob.core.windows.net/replication_folder_delta_tables/\" + tab_name\n",
    "    \n",
    "    try:\n",
    "        # Check if the delta table already exists at the specified path\n",
    "        dbutils.fs.ls(replication_path)\n",
    "        full_load = False\n",
    "    except:\n",
    "        full_load = True\n",
    "\n",
    "    if tab_name in delta_files_list_dict and not full_load:\n",
    "        # Read the existing Delta table\n",
    "        deltaTable = DeltaTable.forPath(spark, replication_path)\n",
    "        \n",
    "        # Load the new data from the input directory (Azure Blob Storage)\n",
    "        df1 = spark.read.load(delta_files_list_dict[tab_name])\n",
    "        \n",
    "        # Load the source definition file\n",
    "        with open('/Workspace/Users/user-email-id/MetaDataManagement_ProjectPro/SourceDefinitionFiles/Delta_Lake/' + tab_name + '.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Construct the merge condition\n",
    "        cond = 'target.' + data['Primary_key'] + '=' + 'updates.' + data['Primary_key']\n",
    "        \n",
    "        # Perform the merge operation\n",
    "        deltaTable.alias('target').merge(df1.alias('updates'), cond).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    elif tab_name in delta_files_list_dict and full_load:\n",
    "        # If full load is required, read the data and save it to the replication folder\n",
    "        df1 = spark.read.load(delta_files_list_dict[tab_name])\n",
    "        df1.write.save(replication_path)\n",
    "\n",
    "    else:\n",
    "        print(f\"No table {tab_name} found with the path mentioned, Please recheck the list mentioned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ae8857-8aa3-475b-a329-68cbdf763201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sqlserver_replication(tab_name):\n",
    "    server_name = \"jdbc:sqlserver://metadatamanagementreplication.database.windows.net\"\n",
    "    database_name = \"metadatamanagementreplication\"\n",
    "    replication_folder=\"\"\n",
    "    url = 'jdbc-url'\n",
    "    username=\"metadatamanagementserver@metadatamanagementreplication\"\n",
    "    password=\"<Azure-sql-server-password>\"\n",
    "    connectionProperties={\"user\":username,\"password\":password,\"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "    #query=\"SELECT table_name FROM INFORMATION_SCHEMA. TABLES WHERE table_type = 'BASE TABLE' and table_name =\"+tab_name+  \";\"\n",
    "    \n",
    "    try:\n",
    "        # Load the table from SQL Server\n",
    "        df1 = spark.read.jdbc(url=url, table=tab_name, properties=connectionProperties)\n",
    "        \n",
    "        # Check if the replication folder exists in Azure Blob Storage\n",
    "        replication_path = f\"wasbs://output@metadatamanagement02.blob.core.windows.net/sql_server/{tab_name}\"\n",
    "        \n",
    "        try:\n",
    "            dbutils.fs.ls(replication_path)\n",
    "            replication_folder = True\n",
    "        except:\n",
    "            replication_folder = False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading table from SQL Server: {e}\")\n",
    "\n",
    "    if replication_folder:\n",
    "        # If replication folder exists, perform Delta table merge operation\n",
    "        deltaTable = DeltaTable.forPath(spark, replication_path)\n",
    "        \n",
    "        # Load the source definition file for the SQL Server table\n",
    "        with open(f'/Workspace/Users/user-email-id/MetaDataManagement_ProjectPro/SourceDefinitionFiles/SQL_server/' + tab_name[4:] + '.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Construct merge condition\n",
    "        cond = f'target.{data[\"Primary_key\"]} = updates.{data[\"Primary_key\"]}'\n",
    "        \n",
    "        # Perform the merge operation\n",
    "        deltaTable.alias('target').merge(df1.alias('updates'), cond).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    else:\n",
    "        # If replication folder doesn't exist, save the data as a new Delta table\n",
    "        df1.write.save(replication_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_replication_path = \"wasbs://output@metadatamanagement02.blob.core.windows.net/replication_folder_csv_tables/\" + file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d02829-1f12-4944-b45b-fb764c5fcc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def  csv_replication(file_name,csv_tables_list_dict):\n",
    "    try:\n",
    "        (dbutils.fs.ls(csv_replication_path))\n",
    "        full_load=False\n",
    "    except:\n",
    "        full_load=True\n",
    "    if file_name in csv_tables_list_dict and not full_load:\n",
    "            deltaTable = DeltaTable.forPath(spark, csv_replication_path)\n",
    "            df1=spark.read.option(\"header\",True).option(\"inferschema\",True).load(csv_tables_list_dict[file_name])\n",
    "            with open('/Workspace/Users/user-email-id/MetaDataManagement_ProjectPro/SourceDefinitionFiles/CSV/'+file_name+'.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            cond='target.'+data['Primary_key']+ '='+ 'updates.'+data['Primary_key']\n",
    "            deltaTable.alias('target').merge(df1.alias('updates'),cond).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "           \n",
    "    elif file_name in csv_tables_list_dict and  full_load:\n",
    "        #print(\"Full load completed\")\n",
    "        df1=spark.read.option(\"inferschema\",True).option(\"header\",True).csv(csv_tables_list_dict[file_name])\n",
    "        df1.write.save(csv_replication_path)\n",
    "               \n",
    "\n",
    "    else:\n",
    "        print(\"No table \",file_name,\" found with the path mentioned, Please recheck the list mentioned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ed7e0b-f2e5-4010-a3da-9f9e028e29b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source_type': 'dbfs_delta_Table', 'table_name': 'Customer'}\n",
      "{'source_type': 'sql_server', 'table_name': 'dbo.Employee'}\n"
     ]
    }
   ],
   "source": [
    "for i in eval(input):\n",
    "    print(i)\n",
    "    if i['source_type']==\"dbfs_delta_Table\":\n",
    "        tab_name=i[\"table_name\"]\n",
    "        delta_file_replication(tab_name,delta_files_list_dict)\n",
    "    elif i['source_type']==\"sql_server\":\n",
    "        sqlserver_replication(i[\"table_name\"])\n",
    "    elif i['source_type']==\"csv\":\n",
    "        csv_replication(i[\"table_name\"],csv_tables_list_dict)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Modular_Replication_Code",
   "widgets": {
    "user_input": {
     "currentValue": "[{'source_type':'dbfs_delta_Table','table_name':'Customer'},{'source_type':'sql_server','table_name':'dbo.Employee'}]",
     "nuid": "b3036bf8-020b-4b22-a173-7cf855bc3764",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "[{'source_type':'dbfs_delta_Table','table_name':'Customer'},{'source_type':'sql_server','table_name':'dbo.employee'}]",
      "label": null,
      "name": "user_input",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "[{'source_type':'dbfs_delta_Table','table_name':'Customer'},{'source_type':'sql_server','table_name':'dbo.employee'}]",
      "label": null,
      "name": "user_input",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
