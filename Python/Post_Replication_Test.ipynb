{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41313a4d-ea1f-4937-a317-692944e76ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e71118-6a1e-4885-b0a8-8093b2988030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Blob Storage details\n",
    "storage_account_name = \"<storage-account-name>\"\n",
    "storage_account_key = \"<storage-account-access-key>\"  \n",
    "container_name = \"input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50edfe9b-7502-4998-86ab-84755d355928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting up the Spark configuration for Azure Blob Storage authentication\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090d3bde-5df0-4aab-9c60-a51da9e22e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class deltatable_test:\n",
    "    def __init__(self,delta_table_name):\n",
    "        self.delta_table_name=delta_table_name\n",
    "        delta_files_list_dict={}\n",
    "        for i in dbutils.fs.ls('dbfs:/databricks-datasets/tpch/delta-001/'):\n",
    "            if i.size ==0:\n",
    "                delta_files_list_dict[i.name[:-1].capitalize()]=i.path\n",
    "        self.delta_files_list_dict=delta_files_list_dict\n",
    "    \n",
    "\n",
    "\n",
    "    def count_test(self):\n",
    "        df_source=spark.read.load( self.delta_files_list_dict[self.delta_table_name])\n",
    "        df_target=spark.read.load(\"wasbs://output@metadatamanagement02.blob.core.windows.net/replication_folder_delta_tables/\"+self.delta_table_name)\n",
    "        if df_source.count()==df_target.count():\n",
    "            return True\n",
    "        else:\n",
    "            print(\"There is difference in counts i.e Source Count =\" ,df_source.count(),\"Target Count is \",df_target.count())\n",
    "            return False\n",
    "    def pk_join(self):\n",
    "\n",
    "        with open('/Workspace/Users/user-email-id/MetaDataManagement_ProjectPro/SourceDefinitionFiles/Delta_Lake/'+self.delta_table_name+'.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "       \n",
    "        df_source=spark.read.load( self.delta_files_list_dict[self.delta_table_name])\n",
    "        df_target=spark.read.load(\"wasbs://output@metadatamanagement02.blob.core.windows.net/replication_folder_delta_tables/\"+self.delta_table_name)\n",
    "        if df_source.join(df_target,data['Primary_key']).count()==df_target.count():\n",
    "            return True\n",
    "        else:\n",
    "            print(\"There is difference in counts i.e Source Count =\" ,df_source.count(),\"Target Count is \",df_target.count())\n",
    "            return False\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee5ca22-4bb1-442f-b98c-a1ed346e2f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class sql_server:\n",
    "    def __init__(self,table_name):\n",
    "        self.table_name=table_name\n",
    "        server_name = \"jdbc:sqlserver://metadatamanagementreplication.database.windows.net\"\n",
    "        database_name = \"metadatamanagementreplication\"\n",
    "        username=\"metadatamanagementserver@metadatamanagementreplication\"\n",
    "        password=\"<azure-sql-server-password>\"\n",
    "        url = '<azure-sql-server-jdbc-url>'\n",
    "        connectionProperties={\"user\":username,\"password\":password,\"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "        \n",
    "    #query=\"SELECT table_name FROM INFORMATION_SCHEMA. TABLES WHERE table_type = 'BASE TABLE' and table_name =\"+tab_name+  \";\"\n",
    "        self.url=url\n",
    "        self.database_name=database_name\n",
    "        self.server_name=server_name\n",
    "        self.connectionProperties=connectionProperties\n",
    "        self.username=username\n",
    "        self.password=password\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def count_test(self):\n",
    "        df_source=spark.read.jdbc(url=self.url,table=self.table_name,properties=self.connectionProperties)\n",
    "        df_target=spark.read.load(\"wasbs://output@metadatamanagement02.blob.core.windows.net/sql_server/\"+self.table_name)\n",
    "        if df_source.count()==df_target.count():\n",
    "            return True\n",
    "        else:\n",
    "            print(\"There is difference in counts i.e Source Count =\" ,df_source.count(),\"Target Count is \",df_target.count())\n",
    "            return False\n",
    "    def pk_join(self):\n",
    "\n",
    "        with open('/Workspace/Users/user-email-id/MetaDataManagement_ProjectPro/SourceDefinitionFiles/SQL_server/'+self.table_name[4:].capitalize()+'.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "        \n",
    "        df_source=spark.read.jdbc(url=self.url,table=self.table_name,properties=self.connectionProperties)\n",
    "        df_target=spark.read.load(\"wasbs://output@metadatamanagement02.blob.core.windows.net/sql_server/\"+self.table_name)\n",
    "        if df_target.count()==df_source.join(df_target,data['Primary_key']).count():\n",
    "            return True\n",
    "        else:\n",
    "            print(\"There is difference in counts i.e Source Count =\" ,df_source.count(),\"Target Count is \",df_target.count())\n",
    "            return False\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a40dde5-16b1-4ad7-86a2-7518b74e0ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "d1=deltatable_test(\"Customer\")\n",
    "d1.pk_join();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2047019-62c0-4605-9972-6e57cf3157c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "c1=sql_server(\"dbo.Employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd82684-bf06-4d5f-a48c-b0f5890a0254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: True"
     ]
    }
   ],
   "source": [
    "c1.count_test()\n",
    "c1.pk_join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f954512-57e7-43c1-9eb4-b848872c16c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = '<logic-app-http-url>'\n",
    "\n",
    "myobj = {\n",
    "    \"emailaddress\": \"<email-id>\",\n",
    "    \"body\": \"triggered from Python Databricks\",\n",
    "    \"subject\": \"sample\"\n",
    "}\n",
    "\n",
    "x = requests.post(url, json = myobj)\n",
    "\n",
    "print(x.text)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Post_Replication_Test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
